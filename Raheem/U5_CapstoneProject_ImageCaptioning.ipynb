{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVMoZwHL4RTh"
      },
      "source": [
        "# Advanced Certification in AIML\n",
        "## A Program by IIIT-H and TalentSprint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EX7dr584we6"
      },
      "source": [
        "## Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjsV_asO4yL_"
      },
      "source": [
        "Automatic Image Captioning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwfwDwfu6MXx"
      },
      "source": [
        "## Objectives\n",
        "Build an image captioning model to generate captions of an image using CNN\n",
        "Dataset Link: Flickr8k_dataset\n",
        "Dataset description: A collection of sentence-based image description\n",
        "\n",
        "● Dataset consists of 8k images in JPEG format with different shapes and sizes.\n",
        "\n",
        "● Images are paired with five different captions which provide clear descriptions of the salient entities and events.\n",
        "\n",
        "● The images were chosen from six different Flickr groups and included a variety of scenes and situations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnkmRa9Uozvd"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "Flickr8k_datase - https://github.com/goodwillyoga/Flickr8k_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLECiqpcdQTQ"
      },
      "source": [
        "## Basic Pytorch packages\n",
        "\n",
        "**torchvision:**  This package is used to load and prepare the dataset. Using this package we can perform/apply transformations on the input data.\n",
        "\n",
        "**transforms:**  This package is  used to perform **preprocessing on images** and operations sequentially.\n",
        "\n",
        "**nn:**  This package provides an easy and modular way to build and train simple or complex neural networks.\n",
        "\n",
        "**optim:** This package is used for  implementing various optimization algorithms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXq346qzqCdh"
      },
      "source": [
        "# Import Libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys, time, os, warnings\n",
        "\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "from pickle import dump\n",
        "from collections import OrderedDict\n",
        "from scipy.spatial.distance import cdist\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn import metrics\n",
        "\n",
        "from keras import models\n",
        "from keras.applications import VGG16\n",
        "#from keras.preprocessing.image import load_img, img_to_array\n",
        "from keras.utils import load_img, img_to_array\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "\n",
        "from IPython import get_ipython\n",
        "ipython = get_ipython()"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBDLQKBJARML",
        "outputId": "5300e798-317c-4bfa-c389-d37b7260cf90"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ipython.magic(\"sx unzip -qq /content/drive/MyDrive/Dataset/Flickr8k_Dataset.zip\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-5QpHDe_YrS",
        "outputId": "23e70f56-529b-4632-8555-8bb23b554cc5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ipython.magic(\"sx unzip -qq /content/drive/MyDrive/Dataset/captions.txt.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMgbNVpGBrTz",
        "outputId": "1dad7912-dfea-4263-acb5-90882e30549d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "images_dir = os.listdir(\"/content/Flicker8k_Dataset\")\n",
        "\n",
        "images_path = './Flicker8k_Dataset/'\n",
        "captions_path = './Flicker8k_Dataset/Flickr_TextData/Flickr8k.token.txt'\n",
        "train_path = './Flicker8k_Dataset/Flickr_TextData/Flickr_8k.trainImages.txt'\n",
        "val_path = './Flicker8k_Dataset/Flickr_TextData/Flickr_8k.devImages.txt'\n",
        "test_path = './Flicker8k_Dataset/Flickr_TextData/Flickr_8k.testImages.txt'\n",
        "\n",
        "# captions = open(captions_path, 'r').read().split(\"\\n\")\n",
        "# x_train = open(train_path, 'r').read().split(\"\\n\")\n",
        "# x_val = open(val_path, 'r').read().split(\"\\n\")\n",
        "# x_test = open(test_path, 'r').read().split(\"\\n\")"
      ],
      "metadata": {
        "id": "OkqLpDm326Pf"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdh-XK6dRw2y"
      },
      "source": [
        "### Defining Transformations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxFVpcd4F79T"
      },
      "source": [
        "# Define transformations for the images\n",
        "\n",
        "from numpy import expand_dims\n",
        "image_size = (128,128)\n",
        "\n",
        "# YOUR CODE HERE for defining Transformation for an image\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(image_size),\n",
        "    transforms.Grayscale(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ls6gI08XH2ak",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "b6222e1d-f2e8-4c3e-efb3-d29bb818030c"
      },
      "source": [
        "# YOUR CODE HERE for the DataLoader\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "\n",
        "train_data = ImageFolder('Flicker8k_Dataset/', transform=transform)\n",
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-bdcceb3b862b>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Flicker8k_Dataset/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0mis_valid_file\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m     ):\n\u001b[0;32m--> 309\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    310\u001b[0m             \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    142\u001b[0m     ) -> None:\n\u001b[1;32m    143\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \"\"\"\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't find any class folder in {directory}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mcls_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Couldn't find any class folder in Flicker8k_Dataset/."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rybt6jhAajgX"
      },
      "source": [
        "## **Stage 2:** Load and Finetune a pre-trained model\n",
        "\n",
        "Load a pretrained model and finetune the appropriate layers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOqy8MZBhDg2"
      },
      "source": [
        "Initialize the device to the available runtime type"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9j8VCAHuhNiv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c2124ee-c4d8-4896-9bd8-efe177d48359"
      },
      "source": [
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = torch.device(\"cuda\")\n",
        "print(device)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## load_vgg16() and run_vgg16()"
      ],
      "metadata": {
        "id": "tVqwMVkFXdWl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_vgg16():\n",
        "    \"\"\"\n",
        "    Load the vgg16 model\n",
        "    \"\"\"\n",
        "    modelvgg = VGG16(include_top=True,weights=None)\n",
        "    modelvgg.load_weights(\"/content/drive/MyDrive/Dataset/vgg16_weights_tf_dim_ordering_tf_kernels.h5\")\n",
        "    # Exclude the last classification layer\n",
        "    modelvgg.layers.pop()\n",
        "    modelvgg = models.Model(inputs=modelvgg.inputs, outputs=modelvgg.layers[-1].output)\n",
        "    modelvgg.summary()\n",
        "    return modelvgg\n",
        "\n",
        "def run_vgg16(dir_Flickr_jpg):\n",
        "    \"\"\"\n",
        "    Generate the image features (4096 elements) from the VGG16 model without the last classification layer\n",
        "    \"\"\"\n",
        "    modelvgg = load_vgg16()\n",
        "    jpgs = os.listdir(dir_Flickr_jpg)\n",
        "    images = OrderedDict()\n",
        "    npix = 224\n",
        "    target_size = (npix,npix,3)\n",
        "    data = np.zeros((len(jpgs),npix,npix,3))\n",
        "    for i,name in enumerate(jpgs):\n",
        "        # load an image from file\n",
        "        filename = dir_Flickr_jpg + '/' + name\n",
        "        image = load_img(filename, target_size=target_size)\n",
        "        # convert the image pixels to a numpy array\n",
        "        image = img_to_array(image)\n",
        "        nimage = preprocess_input(image)\n",
        "\n",
        "        y_pred = modelvgg.predict(nimage.reshape( (1,) + nimage.shape[:3]))\n",
        "        images[name] = y_pred.flatten()\n",
        "\n",
        "    dump(images, open('./images.pkl', 'wb'))\n"
      ],
      "metadata": {
        "id": "v_wpUcv9XQhz"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    dir_Flickr_jpg = \"./Flicker8k_Dataset/\"\n",
        "    run_vgg16(dir_Flickr_jpg)\n",
        "\n",
        "    # images = pd.read_pickle('../data/images.pkl', compression='infer')\n",
        "    # df_txt0 = pd.read_csv('../data/token0.txt', sep='\\t')\n",
        "    # fnames, dcaptions, dimages = link_text_image(df_txt0, images)\n",
        "\n",
        "    # pca = PCA(n_components=3)\n",
        "    # X_pca_3d = pca.fit_transform(dimages)\n",
        "    # plot_elbow(X_pca_3d)\n",
        "    # plot_pca_3d(X_pca_3d[:1000])\n",
        "\n",
        "    # picked_pic = OrderedDict()\n",
        "    # picked_pic[\"purple\"] = [517, 644, 867, 225, 11, 128]\n",
        "    # picked_pic[\"blue\"] = [401,718,591,348,686, 47]\n",
        "    # plot_pca_image(picked_pic, dir_Flickr_jpg)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "VeCyXNOjXwGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Yb9nwUufXnBV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
