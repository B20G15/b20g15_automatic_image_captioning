{"cells":[{"cell_type":"markdown","metadata":{"id":"EHgzY3yTBQIQ"},"source":["#Image Caption Generator using Deep Learning on Flickr8K dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"plaintext"},"id":"3WKGg4ebBQIW"},"outputs":[],"source":["# Import the required libraries\n","import numpy as np\n","from google.colab import drive\n","# data processing, CSV file I / O (e.g. pd.read_csv)\n","import pandas as pd\n","import os\n","import tensorflow as tf\n","from keras.preprocessing.text import Tokenizer\n","from keras.models import Model\n","from keras.layers import Flatten, Dense, LSTM, Dropout, Embedding, Activation\n","from keras.layers import concatenate, BatchNormalization, Input, add\n","from keras.utils import to_categorical, plot_model, pad_sequences\n","from keras.applications.inception_v3 import InceptionV3, preprocess_input\n","import matplotlib.pyplot as plt # for plotting data\n","import cv2"]},{"cell_type":"markdown","metadata":{"id":"_C73uu5bBQIZ"},"source":["#Load the descriptions"]},{"cell_type":"code","source":["drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-JNKsFSKEMm_","executionInfo":{"status":"ok","timestamp":1690710177648,"user_tz":-330,"elapsed":47224,"user":{"displayName":"automatic imagecaption","userId":"15960113411930990751"}},"outputId":"a0bf80d1-c01b-432c-91c5-038401028e53"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["#location of the data\n","data_location =  \"/content/drive/MyDrive/wrk_dir\"\n","!ls $data_location\n","\n","image_path = data_location+\"/flickr8k/images/\"\n","token_path = data_location + '/txt/Flickr8k.token.txt'\n","caption_file = data_location + '/cpt/captions.txt'\n","dev_caption_file = data_location + '/txt/Flickr_8k.devImages.txt'\n","test_caption_file = data_location + '/txt/Flickr_8k.testImages.txt'\n","train_caption_file = data_location + '/txt/Flickr_8k.trainImages.txt'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FzbmT-34E6mO","executionInfo":{"status":"ok","timestamp":1690710438325,"user_tz":-330,"elapsed":403,"user":{"displayName":"automatic imagecaption","userId":"15960113411930990751"}},"outputId":"4950eb5a-3a0e-4922-d5c3-86de6172bf6a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cpt  flickr8k  txt\n"]}]},{"cell_type":"code","source":["#reading the text data\n","run_type = \"Train\"\n","\n","df_8k = pd.read_csv(caption_file)\n","df_dev = pd.read_csv(dev_caption_file)\n","df_test = pd.read_csv(test_caption_file)\n","df_train = pd.read_csv(train_caption_file)\n","\n","print(\"There are {} image to captions in 8K file\".format(len(df_8k)))\n","print(\"There are {} image to captions in Dev\".format(len(df_dev)))\n","print(\"There are {} image to captions in test\".format(len(df_test)))\n","print(\"There are {} image to captions in train\".format(len(df_train)))\n","\n","if run_type == \"Dev\":\n","  cpt_fle_df=pd.merge(df_dev,df_8k, on='image')\n","elif run_type == \"Test\":\n","  cpt_fle_df=pd.merge(df_test,df_8k, on='image')\n","elif run_type == \"Train\":\n","  cpt_fle_df=pd.merge(df_train,df_8k, on='image')\n","else:\n","  cpt_fle_df = df_8k\n","\n","print(\"There are {} image to captions\".format(len(cpt_fle_df)))\n","cpt_fle_df.head(10)"],"metadata":{"id":"D7d8SREuFBhV"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"plaintext"},"id":"c9Bk9AxyBQIZ"},"outputs":[],"source":["def load_description(text):\n","\tmapping = dict()\n","\tfor line in text.split(\"\\n\"):\n","\t\ttoken = line.split(\"\\t\")\n","\t\tif len(line) < 2: # remove short descriptions\n","\t\t\tcontinue\n","\t\timg_id = token[0].split('.')[0] # name of the image\n","\t\timg_des = token[1]\t\t\t # description of the image\n","\t\tif img_id not in mapping:\n","\t\t\tmapping[img_id] = list()\n","\t\tmapping[img_id].append(img_des)\n","\treturn mapping\n"]},{"cell_type":"code","source":["text = open(token_path, 'r', encoding = 'utf-8').read()\n","descriptions = load_description(text)\n","print(descriptions['1000268201_693b08cb0e'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aW7zQxuCMOkO","executionInfo":{"status":"ok","timestamp":1690710445169,"user_tz":-330,"elapsed":383,"user":{"displayName":"automatic imagecaption","userId":"15960113411930990751"}},"outputId":"5e4f6f53-3dc7-46c9-9d47-a70b6899ea03"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['A child in a pink dress is climbing up a set of stairs in an entry way .', 'A girl going into a wooden building .', 'A little girl climbing into a wooden playhouse .', 'A little girl climbing the stairs to her playhouse .', 'A little girl in a pink dress going into a wooden cabin .']\n"]}]},{"cell_type":"markdown","metadata":{"id":"NplHojBXBQIa"},"source":["#Cleaning the text"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"plaintext"},"id":"Vd6XIiheBQIb"},"outputs":[],"source":["def clean_description(desc):\n","\tfor key, des_list in desc.items():\n","\t\tfor i in range(len(des_list)):\n","\t\t\tcaption = des_list[i]\n","\t\t\tcaption = [ch for ch in caption if ch not in string.punctuation]\n","\t\t\tcaption = ''.join(caption)\n","\t\t\tcaption = caption.split(' ')\n","\t\t\tcaption = [word.lower() for word in caption if len(word)>1 and word.isalpha()]\n","\t\t\tcaption = ' '.join(caption)\n","\t\t\tdes_list[i] = caption\n"]},{"cell_type":"code","source":["clean_description(descriptions)\n","descriptions['1000268201_693b08cb0e']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":344},"id":"Kj7qLqtCMlVp","executionInfo":{"status":"error","timestamp":1690710470663,"user_tz":-330,"elapsed":31,"user":{"displayName":"automatic imagecaption","userId":"15960113411930990751"}},"outputId":"1531e9ec-6b79-4b13-ffc8-9e922affa21a"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-2664679f5e68>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclean_description\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdescriptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdescriptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'1000268201_693b08cb0e'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-16-47b334d9e05a>\u001b[0m in \u001b[0;36mclean_description\u001b[0;34m(desc)\u001b[0m\n\u001b[1;32m      3\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdes_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                         \u001b[0mcaption\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdes_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                         \u001b[0mcaption\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mch\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcaption\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mch\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m                         \u001b[0mcaption\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaption\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                         \u001b[0mcaption\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcaption\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-16-47b334d9e05a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      3\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdes_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                         \u001b[0mcaption\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdes_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                         \u001b[0mcaption\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mch\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcaption\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mch\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m                         \u001b[0mcaption\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaption\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                         \u001b[0mcaption\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcaption\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'string' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"WOED7RgfBQIb"},"source":["#Generate the Vocabulary"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"plaintext"},"id":"GdjJb1EnBQIc"},"outputs":[],"source":["def to_vocab(desc):\n","\twords = set()\n","\tfor key in desc.keys():\n","\t\tfor line in desc[key]:\n","\t\t\twords.update(line.split())\n","\treturn words\n","vocab = to_vocab(descriptions)\n"]},{"cell_type":"markdown","metadata":{"id":"AxTi4CfYBQIc"},"source":["#Load the images"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"plaintext"},"id":"iLwiERqDBQId"},"outputs":[],"source":["import glob\n","images = '/kaggle / input / flickr8k / flickr_data / Flickr_Data / Images/'\n","# Create a list of all image names in the directory\n","img = glob.glob(images + '*.jpg')\n","\n","train_path = '/kaggle / input / flickr8k / flickr_data / Flickr_Data / Flickr_TextData / Flickr_8k.trainImages.txt'\n","train_images = open(train_path, 'r', encoding = 'utf-8').read().split(\"\\n\")\n","train_img = [] # list of all images in training set\n","for im in img:\n","\tif(im[len(images):] in train_images):\n","\t\ttrain_img.append(im)\n","\n","# load descriptions of training set in a dictionary. Name of the image will act as ey\n","def load_clean_descriptions(des, dataset):\n","\tdataset_des = dict()\n","\tfor key, des_list in des.items():\n","\t\tif key+'.jpg' in dataset:\n","\t\t\tif key not in dataset_des:\n","\t\t\t\tdataset_des[key] = list()\n","\t\t\tfor line in des_list:\n","\t\t\t\tdesc = 'startseq ' + line + ' endseq'\n","\t\t\t\tdataset_des[key].append(desc)\n","\treturn dataset_des\n","\n","train_descriptions = load_clean_descriptions(descriptions, train_images)\n","print(train_descriptions['1000268201_693b08cb0e'])\n"]},{"cell_type":"markdown","metadata":{"id":"Zhl8FBDXBQIe"},"source":["#Extract the feature vector from all images"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"plaintext"},"id":"gDGqP18YBQIe"},"outputs":[],"source":["from keras.preprocessing.image import load_img, img_to_array\n","def preprocess_img(img_path):\n","\t# inception v3 excepts img in 299 * 299 * 3\n","\timg = load_img(img_path, target_size = (299, 299))\n","\tx = img_to_array(img)\n","\t# Add one more dimension\n","\tx = np.expand_dims(x, axis = 0)\n","\tx = preprocess_input(x)\n","\treturn x\n","\n","def encode(image):\n","\timage = preprocess_img(image)\n","\tvec = model.predict(image)\n","\tvec = np.reshape(vec, (vec.shape[1]))\n","\treturn vec\n","\n","base_model = InceptionV3(weights = 'imagenet')\n","model = Model(base_model.input, base_model.layers[-2].output)\n","# run the encode function on all train images and store the feature vectors in a list\n","encoding_train = {}\n","for img in train_img:\n","\tencoding_train[img[len(images):]] = encode(img)\n"]},{"cell_type":"markdown","metadata":{"id":"4NYiMscKBQIe"},"source":["#Tokenizing the vocabulary"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"plaintext"},"id":"YlWLLc7HBQIe"},"outputs":[],"source":["# list of all training captions\n","all_train_captions = []\n","for key, val in train_descriptions.items():\n","\tfor caption in val:\n","\t\tall_train_captions.append(caption)\n","\n","# consider only words which occur atleast 10 times\n","vocabulary = vocab\n","threshold = 10 # you can change this value according to your need\n","word_counts = {}\n","for cap in all_train_captions:\n","\tfor word in cap.split(' '):\n","\t\tword_counts[word] = word_counts.get(word, 0) + 1\n","\n","vocab = [word for word in word_counts if word_counts[word] >= threshold]\n","\n","# word mapping to integers\n","ixtoword = {}\n","wordtoix = {}\n","\n","ix = 1\n","for word in vocab:\n","\twordtoix[word] = ix\n","\tixtoword[ix] = word\n","\tix += 1\n","\n","# find the maximum length of a description in a dataset\n","max_length = max(len(des.split()) for des in all_train_captions)\n","max_length\n"]},{"cell_type":"markdown","metadata":{"id":"z-xl3j9dBQIf"},"source":["Glove vector embeddings\n","GloVe stands for global vectors for word representation. It is an unsupervised learning algorithm developed by Stanford for generating word embeddings by aggregating global word-word co-occurrence matrix from a corpus. Also, we have 8000 images and each image has 5 captions associated with it. It means we have 30000 examples for training our model. As there are more examples you can also use data generator for feeding input in the form of batches to our model rather than giving all at one time. For simplicity, I’m not using this here.\n","\n","Also, we are going to use an embedding matrix to store the relations between words in our vocabulary. An embedding matrix is a linear mapping of the original space to a real-valued space where entities will have meaningful relationships."]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"plaintext"},"id":"P5qW8EC0BQIg"},"outputs":[],"source":["X1, X2, y = list(), list(), list()\n","for key, des_list in train_descriptions.items():\n","\tpic = train_features[key + '.jpg']\n","\tfor cap in des_list:\n","\t\tseq = [wordtoix[word] for word in cap.split(' ') if word in wordtoix]\n","\t\tfor i in range(1, len(seq)):\n","\t\t\tin_seq, out_seq = seq[:i], seq[i]\n","\t\t\tin_seq = pad_sequences([in_seq], maxlen = max_length)[0]\n","\t\t\tout_seq = to_categorical([out_seq], num_classes = vocab_size)[0]\n","\t\t\t# store\n","\t\t\tX1.append(pic)\n","\t\t\tX2.append(in_seq)\n","\t\t\ty.append(out_seq)\n","\n","X2 = np.array(X2)\n","X1 = np.array(X1)\n","y = np.array(y)\n","\n","# load glove vectors for embedding layer\n","embeddings_index = {}\n","golve_path ='/kaggle / input / glove-global-vectors-for-word-representation / glove.6B.200d.txt'\n","glove = open(golve_path, 'r', encoding = 'utf-8').read()\n","for line in glove.split(\"\\n\"):\n","\tvalues = line.split(\" \")\n","\tword = values[0]\n","\tindices = np.asarray(values[1: ], dtype = 'float32')\n","\tembeddings_index[word] = indices\n","\n","emb_dim = 200\n","emb_matrix = np.zeros((vocab_size, emb_dim))\n","for word, i in wordtoix.items():\n","\temb_vec = embeddings_index.get(word)\n","\tif emb_vec is not None:\n","\t\temb_matrix[i] = emb_vec\n","emb_matrix.shape\n"]},{"cell_type":"markdown","metadata":{"id":"sgrY0LYbBQIi"},"source":["#Define Model"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"plaintext"},"id":"nSjCnz7pBQIi"},"outputs":[],"source":["# define the model\n","ip1 = Input(shape = (2048, ))\n","fe1 = Dropout(0.2)(ip1)\n","fe2 = Dense(256, activation = 'relu')(fe1)\n","ip2 = Input(shape = (max_length, ))\n","se1 = Embedding(vocab_size, emb_dim, mask_zero = True)(ip2)\n","se2 = Dropout(0.2)(se1)\n","se3 = LSTM(256)(se2)\n","decoder1 = add([fe2, se3])\n","decoder2 = Dense(256, activation = 'relu')(decoder1)\n","outputs = Dense(vocab_size, activation = 'softmax')(decoder2)\n","model = Model(inputs = [ip1, ip2], outputs = outputs)\n"]},{"cell_type":"markdown","metadata":{"id":"0AuJw_MSBQIj"},"source":["#Model Training"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"plaintext"},"id":"-7v1b7UsBQIj"},"outputs":[],"source":["model.layers[2].set_weights([emb_matrix])\n","model.layers[2].trainable = False\n","model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n","model.fit([X1, X2], y, epochs = 50, batch_size = 256)\n","# you can increase the number of epochs for better results\n"]},{"cell_type":"markdown","metadata":{"id":"WJH3tDlTBQIk"},"source":["#Predict Output"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"plaintext"},"id":"JRp9BE-9BQIk"},"outputs":[],"source":["def greedy_search(pic):\n","\tstart = 'startseq'\n","\tfor i in range(max_length):\n","\t\tseq = [wordtoix[word] for word in start.split() if word in wordtoix]\n","\t\tseq = pad_sequences([seq], maxlen = max_length)\n","\t\tyhat = model.predict([pic, seq])\n","\t\tyhat = np.argmax(yhat)\n","\t\tword = ixtoword[yhat]\n","\t\tstart += ' ' + word\n","\t\tif word == 'endseq':\n","\t\t\tbreak\n","\tfinal = start.split()\n","\tfinal = final[1:-1]\n","\tfinal = ' '.join(final)\n","\treturn final\n"]},{"cell_type":"markdown","metadata":{"id":"BE2taVLzBQIl"},"source":["Credits: https://www.geeksforgeeks.org/image-caption-generator-using-deep-learning-on-flickr8k-dataset/\n"]}],"metadata":{"language_info":{"name":"python"},"orig_nbformat":4,"colab":{"provenance":[{"file_id":"1EbX28PnOqqc4QA-2FTWV8CLXvzNytkN7","timestamp":1690707505760}]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":0}